{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "設定一些東西齁\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "from transformers import BertTokenizer\n",
    "from ckiptagger import WS, POS, NER\n",
    "import transformers\n",
    "import ckip_transformers\n",
    "from ckip_transformers.nlp import CkipPosTagger\n",
    "from ckip_transformers.nlp import CkipWordSegmenter\n",
    "from ckip_transformers.nlp import CkipNerChunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=BertTokenizer.from_pretrained(\"bert-base-chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'unk_token': '[UNK]',\n",
       " 'sep_token': '[SEP]',\n",
       " 'pad_token': '[PAD]',\n",
       " 'cls_token': '[CLS]',\n",
       " 'mask_token': '[MASK]',\n",
       " 'additional_special_tokens': ['[unused1]']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens =\"[unused1]\"\n",
    "#tokens =['[unused1]']\n",
    "tokenizer.add_tokens(tokens, special_tokens=True)\n",
    "tokenizer.save_pretrained('path/to/save/tokenizer')\n",
    "tokenizer.special_tokens_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ws = WS(\"./data\")\n",
    "#pos = POS(\"./data\")\n",
    "#ner = NER(\"./data\")\n",
    "CKIP_POS=CkipPosTagger(model=\"bert-base\",device=-1)\n",
    "CKIP_WS=CkipWordSegmenter(model=\"bert-base\",device=-1)\n",
    "CKIP_NER=CkipNerChunker(model=\"bert-base\",device=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文本前處理\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正式文本\n",
    "---\n",
    "<span style=\"color:red\">這邊只有先做1000筆 之後再多做然後寫入txt</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Obama大勝美國首位黑人總統'"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"語料\\原始華語.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "    data=f.readlines()\n",
    "formal_text=[]\n",
    "for i in data:\n",
    "    formal_text.append(i[:-1])\n",
    "formal_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64110"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#64110\n",
    "#<25 61308\n",
    "len(formal_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "formal_text=formal_text[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1000/1000 [00:00<00:00, 30471.01it/s]\n",
      "Inference: 100%|██████████| 4/4 [00:58<00:00, 14.62s/it]\n",
      "Tokenization: 100%|██████████| 1000/1000 [00:00<00:00, 52415.04it/s]\n",
      "Inference: 100%|██████████| 4/4 [00:48<00:00, 12.22s/it]\n",
      "Tokenization: 100%|██████████| 1000/1000 [00:00<00:00, 42479.56it/s]\n",
      "Inference: 100%|██████████| 4/4 [01:01<00:00, 15.48s/it]\n"
     ]
    }
   ],
   "source": [
    "#ws_results = ws(data[2])\n",
    "#pos_results = pos(ws_results)\n",
    "#ner_results = ner(ws_results, pos_results)\n",
    "formal_ws=CKIP_WS(formal_text)\n",
    "formal_pos=CKIP_POS(formal_ws)\n",
    "formal_ner=CKIP_NER(formal_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Obama', '大勝', '美國', '首位', '黑人', '總統']"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formal_ws[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NerToken(word='Obama', ner='PERSON', idx=(0, 5)),\n",
       " NerToken(word='美國', ner='GPE', idx=(7, 9)),\n",
       " NerToken(word='首', ner='ORDINAL', idx=(9, 10))]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formal_ner[0]\n",
    "#replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_ner(sent,ner_filter):\n",
    "    filter_ner_sent=[]\n",
    "    for j,word in enumerate(sent):\n",
    "        if word.ner in ner_filter:\n",
    "            filter_ner_sent.append(word.idx)\n",
    "            \n",
    "    return filter_ner_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 5)]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_filter=[\"PERSON\"]\n",
    "formal_ner_person=[filter_by_ner(sent,ner_filter) for sent in formal_ner]\n",
    "formal_ner_person[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "for i in formal_ner_person:\n",
    "    if i!=[]:\n",
    "        text_slice=formal_text[count]\n",
    "        starts, ends = zip(*i)\n",
    "        start_slices = [int(start) for start in starts]\n",
    "        end_slices = [int(end) for end in ends]\n",
    "        person=(f\"{i[0][0]}:{i[0][1]}\")\n",
    "        for x in range(len(start_slices)):\n",
    "            #print(formal_text[count][start_slices[x]:end_slices[x]])\n",
    "            formal_text[count]=formal_text[count].replace(formal_text[count][start_slices[x]:end_slices[x]],\"[unused1]\")\n",
    "            #print(formal_text[count])\n",
    "    count+=1\n",
    "formal_noname=formal_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[unused1]大勝美國首位黑人總統'"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formal_noname[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "#數字\n",
    "formal_tensor=[]\n",
    "#[PAD]\n",
    "formal_token=[]\n",
    "\n",
    "for text in formal_noname:\n",
    "    tensor=tokenizer.encode(text)\n",
    "    if len(tensor)<=30:\n",
    "        tensor_fill=tensor+ [0] * (25 - len(tensor))\n",
    "        formal_tensor.append(tensor_fill)\n",
    "        formal_token.append(tokenizer.decode(tensor_fill))\n",
    "    else:formal_noname.remove(text)\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1, 1920, 1245, 5401, 1751, 7674, 855, 7946, 782, 5244, 5186, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "\n",
      "[CLS] [unused1] 大 勝 美 國 首 位 黑 人 總 統 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "print(formal_tensor[0])\n",
    "print(\"\\n\")\n",
    "print(formal_token[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "非正式文本\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'質感劇本成員都差很多好嗎不要拿腎結石來污辱這群人'"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "informal_text=[] \n",
    "with open(\"Gossiping-QA-Dataset-2_0.csv\", mode=\"r\", encoding=\"utf-8-sig\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    header = next(reader)\n",
    "    for row in reader:\n",
    "        r=row[1].replace(\" \",\"\")\n",
    "        r=r.replace(\",\",\"\")\n",
    "        r=r.replace(\".\",\"\")\n",
    "        r=r.replace(\"。\",\"\")\n",
    "        informal_text.append(r)\n",
    "       \n",
    "informal_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "774114"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(informal_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'質感劇本成員都差很多好嗎不要拿腎結石來污辱這群人'"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "informal_text=informal_text[0:1000]\n",
    "informal_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1000/1000 [00:00<00:00, 11619.25it/s]\n",
      "Inference: 100%|██████████| 4/4 [00:27<00:00,  6.77s/it]\n",
      "Tokenization: 100%|██████████| 1000/1000 [00:00<00:00, 80169.43it/s]\n",
      "Inference: 100%|██████████| 6/6 [00:38<00:00,  6.37s/it]\n",
      "Tokenization: 100%|██████████| 1000/1000 [00:00<00:00, 67601.00it/s]\n",
      "Inference: 100%|██████████| 4/4 [00:25<00:00,  6.49s/it]\n"
     ]
    }
   ],
   "source": [
    "informal_ws=CKIP_WS(informal_text)\n",
    "informal_pos=CKIP_POS(informal_ws)\n",
    "informal_ner=CKIP_NER(informal_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_filter=[\"PERSON\"]\n",
    "informal_ner_person=[filter_by_ner(sent,ner_filter) for sent in informal_ner]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "for i in informal_ner_person:\n",
    "    if i!=[]:\n",
    "        text_slice=informal_text[count]\n",
    "        starts, ends = zip(*i)\n",
    "        start_slices = [int(start) for start in starts]\n",
    "        end_slices = [int(end) for end in ends]\n",
    "        person=(f\"{i[0][0]}:{i[0][1]}\")\n",
    "        for x in range(len(start_slices)):\n",
    "            #print(formal_text[count][start_slices[x]:end_slices[x]])\n",
    "            informal_text[count]=informal_text[count].replace(informal_text[count][start_slices[x]:end_slices[x]],\"[unused1]\")\n",
    "            #print(formal_text[count])\n",
    "    count+=1\n",
    "informal_noname=informal_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "#數字\n",
    "informal_tensor=[]\n",
    "#[PAD]\n",
    "informal_token=[]\n",
    "\n",
    "for text in informal_noname:\n",
    "    tensor=tokenizer.encode(text)\n",
    "    if len(tensor)<=30:\n",
    "        tensor_fill=tensor+ [0] * (25 - len(tensor))\n",
    "        informal_tensor.append(tensor_fill)\n",
    "        informal_token.append(tokenizer.decode(tensor_fill))\n",
    "    else:informal_noname.remove(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 6549, 2697, 1206, 3315, 2768, 1519, 6963, 2345, 2523, 1914, 1962, 1621, 679, 6206, 2897, 5575, 5178, 4767, 889, 3738, 6802, 6857, 5408, 782, 102]\n",
      "\n",
      "\n",
      "[CLS] 質 感 劇 本 成 員 都 差 很 多 好 嗎 不 要 拿 腎 結 石 來 污 辱 這 群 人 [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(informal_tensor[0])\n",
    "print(\"\\n\")\n",
    "print(informal_token[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "27958f768dcca3a929804bc6fbe38aedd46970db20e3e9a2a0a7634f9c4148a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
